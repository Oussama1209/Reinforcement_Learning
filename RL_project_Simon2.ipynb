{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after 200 iterations: 56\n",
      "[-1.         -0.2        -0.25       -1.          0.38285714]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimplePostureBandit:\n",
    "    def __init__(self, k=5, iterations=200, change_probability=0.3, seed=22):\n",
    "        self.k = k  # Number of actions\n",
    "        self.iterations = iterations\n",
    "        self.actions = ['up', 'down', 'right', 'left', 'stay']\n",
    "        self.q_values = np.zeros(k)  # Estimated reward values\n",
    "        self.action_counts = np.zeros(k)  # Count of actions taken\n",
    "        self.total_reward = 0\n",
    "        self.change_probability = change_probability\n",
    "        self.current_state = 'same_posture'  # Initial state\n",
    "        # Set the seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        # Simulate state transition\n",
    "        if np.random.rand() < self.change_probability:\n",
    "            self.current_state = 'changed_posture'\n",
    "        else:\n",
    "            self.current_state = 'same_posture'\n",
    "\n",
    "        # Define rewards based on state and action\n",
    "        if self.current_state == 'changed_posture':\n",
    "            # Let's assume changing posture is generally good\n",
    "            reward = 1 if action != 4 else -1  # Positive reward for any action except 'stay'\n",
    "        else:\n",
    "            reward = -1 if action != 4 else 1  # Negative reward for any action except 'stay'\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def select_action(self):\n",
    "        # Using epsilon-greedy strategy\n",
    "        epsilon = 0.1\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(range(self.k))  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_values)  # Exploit\n",
    "\n",
    "    def update_q_values(self, action, reward):\n",
    "        self.action_counts[action] += 1\n",
    "        alpha = 1 / self.action_counts[action]\n",
    "        self.q_values[action] += alpha * (reward - self.q_values[action])\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.iterations):\n",
    "            action = self.select_action()\n",
    "            reward = self.get_reward(action)\n",
    "            self.update_q_values(action, reward)\n",
    "            self.total_reward += reward\n",
    "\n",
    "        print(f\"Total reward after {self.iterations} iterations: {self.total_reward}\")\n",
    "        return self.q_values\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bandit = SimplePostureBandit()\n",
    "    q = bandit.run()\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.14.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (2.14.0)\n",
      "Requirement already satisfied: tensorflow-probability==0.22.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (0.22.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (18.1.1)\n",
      "Requirement already satisfied: keras<2.15,>=2.14.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (3.3.0)\n",
      "Requirement already satisfied: packaging in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (21.3)\n",
      "Requirement already satisfied: ml-dtypes==0.2.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.2.0)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.4.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.6.3)\n",
      "Requirement already satisfied: tensorboard<2.15,>=2.14 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.4.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (2.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (4.3.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (0.36.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.23.5 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.23.5)\n",
      "Requirement already satisfied: setuptools in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (63.4.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (3.7.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (1.53.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (3.20.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow==2.14.0) (24.3.7)\n",
      "Requirement already satisfied: dm-tree in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.22.0) (0.1.8)\n",
      "Requirement already satisfied: decorator in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.22.0) (5.1.1)\n",
      "Requirement already satisfied: cloudpickle>=1.3 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorflow-probability==0.22.0) (2.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from astunparse>=1.6.0->tensorflow==2.14.0) (0.37.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.31.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.0.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.17.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from packaging->tensorflow==2.14.0) (3.0.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (5.3.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2022.9.24)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (2.0.7)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/simongmur/opt/anaconda3/lib/python3.9/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow==2.14.0) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.14.0 tensorflow-probability==0.22.0\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Probability version:\", tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents.environments as tf_env\n",
    "import tf_agents.specs as tf_specs\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "class FiveArmedBanditEnv(tf_env.py_environment.PyEnvironment):\n",
    "    def __init__(self, user_model_vec, change_probability=0.3, seed=42):\n",
    "        self._num_actions = 5\n",
    "        self._user_model_vec = user_model_vec\n",
    "        self._change_probability = change_probability\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        self._dummy_observation = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        np.random.seed(seed)\n",
    "        super(FiveArmedBanditEnv, self).__init__()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_actions - 1, name='action')\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(self._num_actions,), dtype=np.float32, minimum=0, name='observation')\n",
    "\n",
    "    def _reset(self):\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        return ts.restart(observation=self._dummy_observation)\n",
    "    \n",
    "    def _step(self, action):\n",
    "        fatigue = 1 - np.exp(-self._lambda_fatigue * self._step_count)\n",
    "        adjusted_change_probability = self._change_probability * (1 - fatigue)\n",
    "\n",
    "        if np.random.uniform(0, 1) < adjusted_change_probability:\n",
    "            self.current_state = 'changed_posture'\n",
    "        else:\n",
    "            self.current_state = 'same_posture'\n",
    "\n",
    "        # Define rewards based on the state and action\n",
    "        if self.current_state == 'changed_posture':\n",
    "            reward = 1 if action != 4 else -1  # Positive reward for any action except 'stay'\n",
    "        else:\n",
    "            reward = -1 if action != 4 else 1  # Negative reward for any action except 'stay'\n",
    "\n",
    "        self._step_count += 1\n",
    "        self._total_reward += reward\n",
    "\n",
    "        return ts.transition(observation=self._dummy_observation, reward=reward)\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self._total_reward\n",
    "\n",
    "    def get_step_count(self):\n",
    "        return self._step_count\n",
    "\n",
    "def generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range):\n",
    "    user_profiles = []\n",
    "    for _ in range(num_profiles):\n",
    "        base_change_probability = np.random.uniform(*base_change_probability_range)\n",
    "        lambda_fatigue = np.random.uniform(*lambda_fatigue_range)\n",
    "        user_profiles.append((base_change_probability, lambda_fatigue))\n",
    "    return user_profiles\n",
    "\n",
    "class UCB1Agent:\n",
    "    def __init__(self, num_actions, explore_rate=2.0):\n",
    "        self.num_actions = num_actions\n",
    "        self.explore_rate = explore_rate\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "        self.action_values = np.zeros(num_actions)\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_action(self):\n",
    "        ucb_values = self.action_values + self.explore_rate * np.sqrt(np.log(self.total_steps + 1) / (self.action_counts + 1))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        self.total_steps += 1\n",
    "        self.action_counts[action] += 1\n",
    "        alpha = 1 / self.action_counts[action]\n",
    "        self.action_values[action] += alpha * (reward - self.action_values[action])\n",
    "\n",
    "    def get_current_average_reward(self):\n",
    "        return np.sum(self.action_values * self.action_counts) / np.sum(self.action_counts)\n",
    "\n",
    "class SARSA:\n",
    "    def __init__(self, num_actions, lr=0.1, gamma=0.9):\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.action_values = np.zeros(num_actions)\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return np.argmax(self.action_values)\n",
    "\n",
    "    def update(self, observation, action, reward, next_observation, next_action):\n",
    "        td_target = reward + self.gamma * self.action_values[next_action]\n",
    "        td_error = td_target - self.action_values[action]\n",
    "        self.action_values[action] += self.lr * td_error\n",
    "\n",
    "class LinearFunctionApproximation:\n",
    "    def __init__(self, num_actions, lr=0.1, gamma=0.9, state_dim=5):\n",
    "        self.num_actions = num_actions\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.state_dim = state_dim\n",
    "        self.weights = np.zeros((num_actions, state_dim))\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        q_values = np.dot(self.weights, observation)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    def update(self, observation, action, reward, next_observation, next_action):\n",
    "        q_value = np.dot(self.weights[action], observation)\n",
    "        td_target = reward + self.gamma * np.dot(self.weights[next_action], next_observation)\n",
    "        td_error = td_target - q_value\n",
    "        self.weights[action] += self.lr * td_error * observation\n",
    "\n",
    "def user_simulation_UCB1(user_profiles, num_iterations=500, explore_rate=2.0):\n",
    "    all_average_rewards = []\n",
    "    all_rmse = []\n",
    "\n",
    "    for base_change_probability, lambda_fatigue in user_profiles:\n",
    "        # Create the environment\n",
    "        env = FiveArmedBanditEnv(change_probability=base_change_probability, lambda_fatigue=lambda_fatigue)\n",
    "\n",
    "        # Create the UCB1 agent\n",
    "        agent = UCB1Agent(num_actions=env.action_spec().maximum + 1, explore_rate=explore_rate)\n",
    "\n",
    "        average_rewards = []\n",
    "        rmse = []\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(num_iterations):\n",
    "            # Main stepping\n",
    "            action = agent.select_action()\n",
    "            time_step = env.step(action)\n",
    "            agent.update_estimates(action, time_step.reward)\n",
    "\n",
    "            # Logs for plotting\n",
    "            average_rewards.append(agent.get_current_average_reward())\n",
    "            true_action_value = 2 * base_change_probability - 1\n",
    "            rmse.append(np.sqrt(np.mean((np.subtract(agent.action_values, true_action_value))**2)))\n",
    "\n",
    "        all_average_rewards.append(average_rewards)\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        print(f\"User profile with base change probability {base_change_probability:.2f} and lambda fatigue {lambda_fatigue:.2f}\")\n",
    "        print(\"True values: {}\".format(true_action_value))\n",
    "        print(\"Value estimates: {}\".format(agent.action_values))\n",
    "        print(\"Action counts: {}\".format(agent.action_counts))\n",
    "\n",
    "    return all_average_rewards, all_rmse\n",
    "\n",
    "def user_simulation_SARSA(user_profiles, num_iterations=500, lr=0.1, gamma=0.9):\n",
    "    all_average_rewards = []\n",
    "    all_rmse = []\n",
    "\n",
    "    for base_change_probability, lambda_fatigue in user_profiles:\n",
    "        # Create the environment\n",
    "        env = FiveArmedBanditEnv(user_model_vec=None, change_probability=base_change_probability)\n",
    "        agent = SARSA(num_actions=env.action_spec().maximum + 1, lr=lr, gamma=gamma)\n",
    "        \n",
    "        average_rewards = []\n",
    "        rmse = []\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(num_iterations):\n",
    "            # Main stepping\n",
    "            action = agent.select_action(env.current_time_step().observation)\n",
    "            time_step = env.step(action)\n",
    "            next_action = agent.select_action(time_step.observation)\n",
    "            agent.update(\n",
    "                observation=env.current_time_step().observation,\n",
    "                action=action,\n",
    "                reward=time_step.reward,\n",
    "                next_observation=time_step.observation,\n",
    "                next_action=next_action\n",
    "            )\n",
    "\n",
    "            # Logs for plotting\n",
    "            average_rewards.append(env.get_total_reward() / env.get_step_count())\n",
    "            true_action_value = 2 * base_change_probability - 1\n",
    "            rmse.append(np.sqrt(np.mean((np.subtract(agent.action_values, true_action_value))**2)))\n",
    "\n",
    "        all_average_rewards.append(average_rewards)\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        print(f\"User profile with base change probability {base_change_probability:.2f} and lambda fatigue {lambda_fatigue:.2f}\")\n",
    "        print(\"True values: {}\".format(true_action_value))\n",
    "        print(\"Value estimates: {}\".format(agent.action_values))\n",
    "        print(\"Action counts: N/A (SARSA agent does not keep track of action counts)\")\n",
    "    return all_average_rewards, all_rmse\n",
    "\n",
    "def user_simulation_LFA(user_profiles, num_iterations=500, lr=0.1, gamma=0.9):\n",
    "    all_average_rewards = []\n",
    "    all_rmse = []\n",
    "\n",
    "    for base_change_probability, lambda_fatigue in user_profiles:\n",
    "        # Create the environment\n",
    "        env = FiveArmedBanditEnv(user_model_vec=None, change_probability=base_change_probability)\n",
    "\n",
    "        agent = LinearFunctionApproximation(num_actions=env.action_spec().maximum + 1, lr=lr, gamma=gamma)\n",
    "\n",
    "        average_rewards = []\n",
    "        rmse = []\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(num_iterations):\n",
    "            # Main stepping\n",
    "            action = agent.select_action(env.current_time_step().observation)\n",
    "            time_step = env.step(action)\n",
    "            next_action = agent.select_action(time_step.observation)\n",
    "            agent.update(\n",
    "                observation=env.current_time_step().observation,\n",
    "                action=action,\n",
    "                reward=time_step.reward,\n",
    "                next_observation=time_step.observation,\n",
    "                next_action=next_action\n",
    "            )\n",
    "\n",
    "            # Logs for plotting\n",
    "            average_rewards.append(env.get_total_reward() / env.get_step_count())\n",
    "            true_action_value = 2 * base_change_probability - 1\n",
    "            rmse.append(np.sqrt(np.mean((np.subtract(agent.action_values, true_action_value))**2)))\n",
    "\n",
    "        all_average_rewards.append(average_rewards)\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        print(f\"User profile with base change probability {base_change_probability:.2f} and lambda fatigue {lambda_fatigue:.2f}\")\n",
    "        print(\"True values: {}\".format(true_action_value))\n",
    "        print(\"Value estimates: {}\".format(agent.action_values))\n",
    "        print(\"Action counts: N/A (LFA agent does not keep track of action counts)\")\n",
    "    return all_average_rewards, all_rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_profiles = 30 # We want enough profiles to see the variance in the results but not too many to stay feasible in the real experiment\n",
    "    base_change_probability_range = (0.1, 0.5) # At minimum, the user changes posture 10% of the time and at maximum 50% of the time\n",
    "    lambda_fatigue_range = (0.005, 0.1) # In the paper the values chosen were 0.01, 0.03 and 0.05\n",
    "    \n",
    "    # UCB1\n",
    "    user_profiles = generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range)\n",
    "    average_rewards, rmse = user_simulation_UCB1(user_profiles)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for i, (avg_rewards, rms) in enumerate(zip(average_rewards, rmse)):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(avg_rewards, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward over Time')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(rms, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE over Time')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # SARSA\n",
    "    user_profiles = generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range)\n",
    "    average_rewards, rmse = user_simulation_SARSA(user_profiles)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for i, (avg_rewards, rms) in enumerate(zip(average_rewards, rmse)):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(avg_rewards, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward over Time')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(rms, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE over Time')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # LFA\n",
    "    user_profiles = generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range)\n",
    "    average_rewards, rmse = user_simulation_LFA(user_profiles)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for i, (avg_rewards, rms) in enumerate(zip(average_rewards, rmse)):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(avg_rewards, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward over Time')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(rms, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE over Time')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
