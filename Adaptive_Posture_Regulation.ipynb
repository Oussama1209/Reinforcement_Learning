{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env \n",
    "from gym import spaces \n",
    "from IPython.display import clear_output\n",
    "import random \n",
    "import numpy as np \n",
    "import random\n",
    "import pprint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global constants\n",
    "\n",
    "# game board values\n",
    "NOTHING = 0\n",
    "USER = 1\n",
    "# states\n",
    "CHANGE = 2\n",
    "NO_CHANGE= 3\n",
    "# action values\n",
    "UP = 0\n",
    "DOWN = 1\n",
    "LEFT = 2\n",
    "RIGHT = 3\n",
    "# STAY = 4\n",
    "\n",
    "NUM_ACTIONS = 4\n",
    "\n",
    "# game board size\n",
    "BOARD_SIZE = 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>User Profile Generation for RL Project</h3>\n",
    "\n",
    "<p>This code snippet generates user profiles and simulates user behavior to predict posture changes based on window position changes on the screen. It includes functions to create profiles for individual users and multiple users, incorporating parameters for action probabilities and fatigue. The <code>User</code> class models each user's behavior, adjusting state change probabilities based on fatigue over time.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_profile(num_env_states, base_change_probability_range, lambda_fatigue_range, num_actions=4):\n",
    "    user_profile = []\n",
    "    user_lambda_fatigue = np.random.uniform(*lambda_fatigue_range)\n",
    "    \n",
    "    for _ in range(num_env_states):\n",
    "        state_probabilities = []\n",
    "        for _ in range(num_actions):\n",
    "            base_change_probability_state = np.random.uniform(*base_change_probability_range)\n",
    "            state_probabilities.append(base_change_probability_state)\n",
    "        user_profile.append(state_probabilities)\n",
    "    \n",
    "    return user_profile, user_lambda_fatigue\n",
    "\n",
    "def generate_all_users_profile(num_users, base_change_probability_range, lambda_fatigue_range):\n",
    "    all_users_profiles = []\n",
    "    all_users_lambda_fatigue = []\n",
    "    \n",
    "    for _ in range(num_users):\n",
    "        user_profile, user_lambda_fatigue = generate_user_profile(36, base_change_probability_range, lambda_fatigue_range)\n",
    "        all_users_profiles.append(user_profile)\n",
    "        all_users_lambda_fatigue.append(user_lambda_fatigue)\n",
    "    \n",
    "    return all_users_profiles, all_users_lambda_fatigue\n",
    "    \n",
    "class User():\n",
    "    def __init__(self, user_profile, lambda_fatigue):\n",
    "        self._user_profile = user_profile\n",
    "        self._lambda_fatigue = lambda_fatigue\n",
    "        self._step_count = 0\n",
    "\n",
    "    def fatigue_adjusted(self):\n",
    "        return 1 - np.exp(-self._lambda_fatigue * self._step_count)\n",
    "    \n",
    "    def change_state(self, state, action, previous_action):\n",
    "        fatigue = self.fatigue_adjusted()\n",
    "        adjusted_change_probability = self._user_profile[state][action] * (1 - fatigue)\n",
    "        self._step_count += 1\n",
    "        if np.random.uniform(0, 1) <= adjusted_change_probability:\n",
    "            return CHANGE\n",
    "        else:\n",
    "            return NO_CHANGE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Environment Setup for `ScreenEnv`</h2>\n",
    "<p>The `ScreenEnv` class sets up a simulated environment as a 6x6 grid, representing the screen. The key components are:</p>\n",
    "<h3>Grid Representation</h3>\n",
    "<p>\n",
    "The environment is a flattened 6x6 grid (36 positions).\n",
    "<ul>\n",
    "  <li><strong>Initial State:</strong> Each position in the grid starts as `NOTHING` (indicating an empty or black screen).</li>\n",
    "  <li><strong>User Position:</strong> The user starts at a specific position (default is position 4) on this grid.</li>\n",
    "  <li><strong>User State:</strong> Tracks whether the user's posture has changed or not (`NO_CHANGE` initially).</li>\n",
    "</ul>\n",
    "</p>\n",
    "<h3>State and Action Spaces</h3>\n",
    "<p>\n",
    "<ul>\n",
    "  <li><strong>State Space:</strong> A 36-element numpy array representing the grid, where each element can have values from 0 to 3.</li>\n",
    "  <ul>\n",
    "    <li><code>0</code>: NOTHING (empty position)</li>\n",
    "    <li><code>1</code>: USER (current position of the user)</li>\n",
    "  </ul>\n",
    "  <li><strong>Action Space:</strong> Defines four possible actions the user can take:</li>\n",
    "  <ul>\n",
    "    <li><code>0</code>: Move up</li>\n",
    "    <li><code>1</code>: Move down</li>\n",
    "    <li><code>2</code>: Move left</li>\n",
    "    <li><code>3</code>: Move right</li>\n",
    "  </ul>\n",
    "</ul>\n",
    "</p>\n",
    "<h3>Action Validation</h3>\n",
    "<p>\n",
    "<strong>Action Constraints:</strong> Checks ensure that the user does not move out of the grid boundaries.\n",
    "<ul>\n",
    "  <li>Moving up is valid if the new position is within the grid.</li>\n",
    "  <li>Moving down is valid if the new position is within the grid.</li>\n",
    "  <li>Moving left is valid if the user is not in the leftmost column.</li>\n",
    "  <li>Moving right is valid if the user is not in the rightmost column.</li>\n",
    "</ul>\n",
    "</p>\n",
    "<h3>User Interaction and State Update</h3>\n",
    "<p>\n",
    "<strong>Step Method:</strong> Executes an action and updates the user's position and state.\n",
    "<ul>\n",
    "  <li><strong>User Movement:</strong> The user's position is updated based on the action taken if it's valid.</li>\n",
    "  <li><strong>State Change:</strong> The user's posture change is determined by their profile and fatigue.</li>\n",
    "  <li><strong>Rewards:</strong> Rewards are assigned based on whether the user's posture changes (`CHANGE` gives a +1 reward, `NO_CHANGE` gives a -1 reward).</li>\n",
    "</ul>\n",
    "</p>\n",
    "<h3>Reset and Render</h3>\n",
    "<p>\n",
    "<ul>\n",
    "  <li><strong>Reset Method:</strong> Resets the environment to its initial state, placing the user at a random position on the grid.</li>\n",
    "  <li><strong>Render Method:</strong> Prints the current state of the grid and cumulative reward (for debugging purposes).</li>\n",
    "</ul>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScreenEnv(Env):\n",
    "    def __init__(self, user, max_iter=200):\n",
    "        # custom class variable used to display the reward earned\n",
    "        self.cumulative_reward = 0\n",
    "        self.iter_number = 0\n",
    "        self.max_iter = max_iter\n",
    "        #\n",
    "        # set the initial state to a flattened 6x6 grid with a randomly\n",
    "        # placed user\n",
    "        #\n",
    "        self.state = [NOTHING] * 36\n",
    "        self.user_position = 4\n",
    "        self.user_state = NO_CHANGE\n",
    "        self.user = user\n",
    "        \n",
    "        self.state[self.user_position] = USER\n",
    "        # convert the python array into a numpy array \n",
    "        self.state = np.array(self.state, dtype=np.int16)\n",
    "        # observation space (valid ranges for observations in the state)\n",
    "        self.observation_space = spaces.Box(0, 3, [36,], dtype=np.int16)\n",
    "        # valid actions:\n",
    "        #   0 = up\n",
    "        #   1 = down\n",
    "        #   2 = left\n",
    "        #   3 = right\n",
    "        self.action_space = spaces.Discrete(5)\n",
    "\n",
    "    def is_valid_action(self, action):\n",
    "        if action == UP:\n",
    "            return (self.user_position - 6) >= 0\n",
    "        elif action == DOWN:\n",
    "            return (self.user_position + 6) < 36\n",
    "        elif action == LEFT:\n",
    "            return (self.user_position % 6) != 0\n",
    "        elif action == RIGHT:\n",
    "            return (self.user_position % 6) != 5\n",
    "        return False\n",
    "    \n",
    "    def step(self, action):\n",
    "        # placeholder for debugging information\n",
    "        info = {}\n",
    "        # set default values for done, reward, and the user position\n",
    "        # before taking the action\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        if self.iter_number >= self.max_iter:\n",
    "            done = True\n",
    "            return self.state, reward, done, info, True\n",
    "\n",
    "        previous_position = self.user_position\n",
    "\n",
    "        # take the action by moving the user\n",
    "        if self.is_valid_action(action):\n",
    "            if action == UP:\n",
    "                self.user_position -= 6\n",
    "            elif action == DOWN:\n",
    "                self.user_position += 6\n",
    "            elif action == LEFT:\n",
    "                self.user_position -= 1\n",
    "            elif action == RIGHT:\n",
    "                self.user_position += 1\n",
    "            self.iter_number += 1\n",
    "        else:\n",
    "            return self.state, reward, done, info, False\n",
    "\n",
    "        # give the state of user\n",
    "        self.user_state = self.user.change_state(previous_position, action, self.user_state)\n",
    "\n",
    "        # set reward based on the user's state\n",
    "        if self.user_state == CHANGE:\n",
    "            reward = 1.0\n",
    "        elif self.user_state == NO_CHANGE:\n",
    "            reward = -1.0\n",
    "        else:\n",
    "            reward = 0.0  # Default reward for other cases\n",
    "\n",
    "        # Update the environment state\n",
    "        if not done:\n",
    "            # update the user position\n",
    "            self.state[previous_position] = NOTHING\n",
    "            self.state[self.user_position] = USER\n",
    "\n",
    "        self.cumulative_reward += reward\n",
    "\n",
    "        return self.state, reward, done, info, True\n",
    "    \n",
    "    def reset(self):\n",
    "        self.cumulative_reward = 0\n",
    "        self.iter_number = 0\n",
    "        #\n",
    "        # set the initial state to a flattened 6x6 grid with a randomly \n",
    "        # placed user\n",
    "        #\n",
    "        self.state = [NOTHING] * 36\n",
    "        self.user_position = random.randrange(0, 36)\n",
    "        \n",
    "        self.state[self.user_position] = USER\n",
    "        # convert the python array into a numpy array \n",
    "        # (needed since Gym expects the state to be this way)\n",
    "        self.state = np.array(self.state, dtype=np.int16)\n",
    "        # print(f\"Environment reset. User Position: {self.user_position}\")\n",
    "        # pp = pprint.PrettyPrinter(indent=4)\n",
    "        # reshaped_state = self.state.reshape(6, 6)\n",
    "        # pp.pprint(reshaped_state)\n",
    "        return self.state\n",
    "    \n",
    "    def render(self):\n",
    "        pp = pprint.PrettyPrinter(indent=4)\n",
    "        # pp.pprint({\"state\": self.state.reshape(6, 6), \"cumulative_reward\": self.cumulative_reward})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Double Q-Learning Agent</h2>\n",
    "<p>The <code>DoubleQLearningAgent</code> class implements the Double Q-Learning algorithm, which is effective in stochastic environments by reducing the overestimation bias that can occur with standard Q-learning.</p>\n",
    "<h3>Key Components</h3>\n",
    "<ul>\n",
    "  <li><strong>Initialization:</strong> Sets up the agent with learning parameters and initializes two Q-tables (<code>q_table1</code> and <code>q_table2</code>).</li>\n",
    "  <li><strong>Choosing Actions:</strong> Uses an epsilon-greedy strategy to balance exploration and exploitation.</li>\n",
    "  <li><strong>Learning:</strong> Updates the Q-values using two Q-tables to decouple action selection from evaluation:</li>\n",
    "</ul>\n",
    "<p style=\"text-align: center;\">\n",
    "<code>Q1(s, a) &larr; Q1(s, a) + &alpha; [r + &gamma; Q2(s', argmax<sub>a'</sub> Q1(s', a')) - Q1(s, a)]</code>\n",
    "<br>\n",
    "<code>Q2(s, a) &larr; Q2(s, a) + &alpha; [r + &gamma; Q1(s', argmax<sub>a'</sub> Q2(s', a')) - Q2(s, a)]</code>\n",
    "</p>\n",
    "<ul>\n",
    "  <li><strong>Exploration Decay:</strong> Gradually reduces the exploration rate over time.</li>\n",
    "</ul>\n",
    "<p>This algorithm is added because it performs better in stochastic environments by reducing overestimation bias and providing more stable learning.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningAgent:\n",
    "    def __init__(self, env, learning_rate=0.05, discount_factor=0.99, exploration_rate=0.2, exploration_decay=0.99):\n",
    "        self.env = env\n",
    "        self.q_table1 = np.zeros((36, env.action_space.n))\n",
    "        self.q_table2 = np.zeros((36, env.action_space.n))\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_index = self.get_state_index(state)\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            action = self.env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(self.q_table1[state_index] + self.q_table2[state_index])  # Exploit learned values\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state_index = self.get_state_index(state)\n",
    "        next_state_index = self.get_state_index(next_state)\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            predict = self.q_table1[state_index, action]\n",
    "            a_prime = np.argmax(self.q_table1[next_state_index])\n",
    "            target = reward + self.discount_factor * self.q_table2[next_state_index, a_prime]\n",
    "            self.q_table1[state_index, action] += self.learning_rate * (target - predict)\n",
    "        else:\n",
    "            predict = self.q_table2[state_index, action]\n",
    "            a_prime = np.argmax(self.q_table2[next_state_index])\n",
    "            target = reward + self.discount_factor * self.q_table1[next_state_index, a_prime]\n",
    "            self.q_table2[state_index, action] += self.learning_rate * (target - predict)\n",
    "\n",
    "    def update_exploration_rate(self):\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "    def get_state_index(self, state):\n",
    "        # Convert the state representation to a single integer index if necessary\n",
    "        return np.argmax(state)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return np.argmax(self.q_table1 + self.q_table2, axis=1)\n",
    "\n",
    "    def predict_reward(self, state, action):\n",
    "        state_index = self.get_state_index(state)\n",
    "        return (self.q_table1[state_index, action] + self.q_table2[state_index, action]) / 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Expected SARSA Agent</h2>\n",
    "<p>The <code>ExpectedSARSAAgent</code> class implements the Expected SARSA algorithm for learning in a stochastic environment, which is more effective than traditional methods in such settings.</p>\n",
    "<h3>Key Components</h3>\n",
    "<ul>\n",
    "  <li><strong>Initialization:</strong> Sets up the agent with learning parameters and initializes the Q-table.</li>\n",
    "  <li><strong>Choosing Actions:</strong> Uses an epsilon-greedy strategy to balance exploration and exploitation.</li>\n",
    "  <li><strong>Learning:</strong> Updates the Q-values based on the expected value of the next state's Q-values using the formula:</li>\n",
    "</ul>\n",
    "<p style=\"text-align: center;\">\n",
    "<code>Q(s, a) &larr; Q(s, a) + &alpha; [r + &gamma; &sum;_a' &pi;(a'|s')Q(s', a') - Q(s, a)]</code>\n",
    "</p>\n",
    "<ul>\n",
    "  <li><strong>Exploration Decay:</strong> Gradually reduces the exploration rate over time.</li>\n",
    "</ul>\n",
    "<p>This algorithm is added because it performs better in stochastic environments by considering the expected value of the next state.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpectedSARSAAgent:\n",
    "    def __init__(self, env, learning_rate=0.1, discount_factor=0.99, exploration_rate=1.0, exploration_decay=0.99):\n",
    "        self.env = env\n",
    "        self.q_table = np.zeros((36, env.action_space.n))  # Adjust size based on state space\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.exploration_rate = exploration_rate\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        state_index = self.get_state_index(state)\n",
    "        if random.uniform(0, 1) < self.exploration_rate:\n",
    "            action = self.env.action_space.sample()  # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(self.q_table[state_index])  # Exploit learned values\n",
    "        return action\n",
    "\n",
    "    def learn(self, state, action, reward, next_state):\n",
    "        state_index = self.get_state_index(state)\n",
    "        next_state_index = self.get_state_index(next_state)\n",
    "        \n",
    "        # Calculate expected Q-value for the next state\n",
    "        policy = self.get_policy_for_state(next_state_index)\n",
    "        expected_q = np.sum(policy * self.q_table[next_state_index])\n",
    "        \n",
    "        # Update rule for Expected SARSA\n",
    "        target = reward + self.discount_factor * expected_q\n",
    "        self.q_table[state_index, action] += self.learning_rate * (target - self.q_table[state_index, action])\n",
    "\n",
    "    def update_exploration_rate(self):\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "\n",
    "    def get_state_index(self, state):\n",
    "        # Convert the state representation to a single integer index if necessary\n",
    "        return np.argmax(state)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return np.argmax(self.q_table, axis=1)\n",
    "\n",
    "    def get_policy_for_state(self, state_index):\n",
    "        q_values = self.q_table[state_index]\n",
    "        max_q_value = np.max(q_values)\n",
    "        policy = np.zeros_like(q_values)\n",
    "        count_max_q_values = np.sum(q_values == max_q_value)\n",
    "        policy[q_values == max_q_value] = 1.0 / count_max_q_values\n",
    "        return policy\n",
    "\n",
    "    def predict_reward(self, state, action):\n",
    "        state_index = self.get_state_index(state)\n",
    "        return self.q_table[state_index, action]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RMSE\n",
    "def calculate_rmse(q_table1, q_table2):\n",
    "    return np.sqrt(np.mean((q_table1 - q_table2) ** 2))\n",
    "\n",
    "# Function to calculate rolling average\n",
    "def rolling_average(data, window_size):\n",
    "    return np.convolve(data, np.ones(window_size) / window_size, mode='valid')\n",
    "\n",
    "# Function to train a Double Q-Learning agent\n",
    "def train_double_q_learning_agent(env, learning_rate, exploration_rate, exploration_decay=0.99, episodes=10):\n",
    "    agent = DoubleQLearningAgent(env, learning_rate, discount_factor=0.99, exploration_rate=exploration_rate, exploration_decay=exploration_decay)\n",
    "    cumulative_rewards = []\n",
    "    rmse_values = []\n",
    "\n",
    "    cumulative_reward = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info, valid = env.step(action)\n",
    "\n",
    "            if not valid:\n",
    "                continue  # If the action was invalid, choose another action\n",
    "\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += reward\n",
    "            cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "            # Calculate RMSE\n",
    "            rmse = calculate_rmse(agent.q_table1, agent.q_table2)\n",
    "            rmse_values.append(rmse)\n",
    "\n",
    "        # Update exploration rate\n",
    "        agent.update_exploration_rate()\n",
    "\n",
    "    return cumulative_rewards, rmse_values\n",
    "\n",
    "# Function to train a Double Q-Learning agent for multiple users, averaging results over multiple runs\n",
    "def train_all_users_double_q_learning(num_users, learning_rate, exploration_rate, exploration_decay, episodes, runs=3):\n",
    "    all_cumulative_rewards = []\n",
    "    all_rmse_values = []\n",
    "\n",
    "    for _ in range(num_users):\n",
    "        cumulative_rewards_runs = []\n",
    "        rmse_values_runs = []\n",
    "\n",
    "        for _ in range(runs):\n",
    "            # Initialize the environment and user profile here\n",
    "            user_profile, lambda_fatigue = generate_user_profile(36, base_change_probability_range, lambda_fatigue_range)\n",
    "            user = User(user_profile, lambda_fatigue)\n",
    "            env = ScreenEnv(user, max_iter=10)\n",
    "            \n",
    "            # Train the agent\n",
    "            cumulative_rewards, rmse_values = train_double_q_learning_agent(env, learning_rate, exploration_rate, exploration_decay, episodes)\n",
    "            cumulative_rewards_runs.append(cumulative_rewards)\n",
    "            rmse_values_runs.append(rmse_values)\n",
    "        \n",
    "        # Average the results over multiple runs\n",
    "        avg_cumulative_rewards = np.mean(cumulative_rewards_runs, axis=0)\n",
    "        avg_rmse_values = np.mean(rmse_values_runs, axis=0)\n",
    "        all_cumulative_rewards.append(avg_cumulative_rewards)\n",
    "        all_rmse_values.append(avg_rmse_values)\n",
    "    \n",
    "    return all_cumulative_rewards, all_rmse_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train an Expected SARSA agent\n",
    "def train_expected_sarsa_agent(env, learning_rate, exploration_rate, exploration_decay=0.99, episodes=10):\n",
    "    agent = ExpectedSARSAAgent(env, learning_rate, discount_factor=0.99, exploration_rate=exploration_rate, exploration_decay=exploration_decay)\n",
    "    cumulative_rewards = []\n",
    "\n",
    "    cumulative_reward = 0\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, info, valid = env.step(action)\n",
    "\n",
    "            if not valid:\n",
    "                continue  # If the action was invalid, choose another action\n",
    "\n",
    "            agent.learn(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "\n",
    "            # Update cumulative reward\n",
    "            cumulative_reward += reward\n",
    "            cumulative_rewards.append(cumulative_reward)\n",
    "\n",
    "        # Update exploration rate\n",
    "        agent.update_exploration_rate()\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "# Function to train an Expected SARSA agent for multiple users, averaging results over multiple runs\n",
    "def train_all_users_expected_sarsa(num_users, learning_rate, exploration_rate, exploration_decay, episodes, runs=3):\n",
    "    all_cumulative_rewards = []\n",
    "\n",
    "    for _ in range(num_users):\n",
    "        cumulative_rewards_runs = []\n",
    "\n",
    "        for _ in range(runs):\n",
    "            # Initialize the environment and user profile here\n",
    "            user_profile, lambda_fatigue = generate_user_profile(36, base_change_probability_range, lambda_fatigue_range)\n",
    "            user = User(user_profile, lambda_fatigue)\n",
    "            env = ScreenEnv(user, max_iter=10)\n",
    "            \n",
    "            # Train the agent\n",
    "            cumulative_rewards = train_expected_sarsa_agent(env, learning_rate, exploration_rate, exploration_decay, episodes)\n",
    "            cumulative_rewards_runs.append(cumulative_rewards)\n",
    "        \n",
    "        # Average the results over multiple runs\n",
    "        avg_cumulative_rewards = np.mean(cumulative_rewards_runs, axis=0)\n",
    "        all_cumulative_rewards.append(avg_cumulative_rewards)\n",
    "    \n",
    "    return all_cumulative_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementation of Double Q-Learning with Fatigue</h3>\n",
    "\n",
    "Perform a grid search to find the optimal parameters for this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_users = 15\n",
    "learning_rate = 0.05\n",
    "exploration_rate = 0.2\n",
    "exploration_decay = 0.99\n",
    "episodes = 100\n",
    "base_change_probability_range = (0.3, 0.7)  # Example range, replace with actual range\n",
    "lambda_fatigue_range = (0.0001, 0.0002)  # Example range, replace with actual range\n",
    "\n",
    "all_cumulative_rewards_double_q, all_rmse_values_double_q = train_all_users_double_q_learning(num_users, learning_rate, exploration_rate, exploration_decay, episodes)\n",
    "\n",
    "# Plot cumulative rewards and RMSE values for all users using Double Q-Learning\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# Cumulative Rewards Plot for Double Q-Learning\n",
    "for i, rewards in enumerate(all_cumulative_rewards_double_q):\n",
    "    ax1.plot(rewards)\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Cumulative Rewards')\n",
    "ax1.set_title('Cumulative Rewards for All Users for Double Q-Learning with Fatigue')\n",
    "ax1.legend()\n",
    "\n",
    "# RMSE Plot for Double Q-Learning\n",
    "for i, rmse in enumerate(all_rmse_values_double_q):\n",
    "    ax2.plot(rmse)\n",
    "ax2.set_xlabel('Iterations')\n",
    "ax2.set_ylabel('RMSE')\n",
    "ax2.set_title('RMSE for All Users for Double Q-Learningm with Fatigue')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Implementation of Expected SARSA with Fatigue</h3>\n",
    "\n",
    "Perform a grid search to find the optimal parameters for this algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "num_users = 15\n",
    "learning_rate = 0.05\n",
    "exploration_rate = 0.2\n",
    "exploration_decay = 0.99\n",
    "episodes = 100\n",
    "base_change_probability_range = (0.3, 0.7)  # Example range, replace with actual range\n",
    "lambda_fatigue_range = (0.0001, 0.0002)  # Example range, replace with actual range\n",
    "\n",
    "all_cumulative_rewards_expected_sarsa = train_all_users_expected_sarsa(num_users, learning_rate, exploration_rate, exploration_decay, episodes)\n",
    "\n",
    "# Plot cumulative rewards for all users using Expected SARSA\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Cumulative Rewards Plot for Expected SARSA\n",
    "for i, rewards in enumerate(all_cumulative_rewards_expected_sarsa):\n",
    "    ax1.plot(rewards)\n",
    "ax1.set_xlabel('Iterations')\n",
    "ax1.set_ylabel('Cumulative Rewards')\n",
    "ax1.set_title('Cumulative Rewards for All Users for Expected SARSA with Fatigue')\n",
    "ax1.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
