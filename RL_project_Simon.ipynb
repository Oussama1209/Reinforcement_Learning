{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward after 200 iterations: 56\n",
      "[-1.         -0.2        -0.25       -1.          0.38285714]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimplePostureBandit:\n",
    "    def __init__(self, k=5, iterations=200, change_probability=0.3, seed=22):\n",
    "        self.k = k  # Number of actions\n",
    "        self.iterations = iterations\n",
    "        self.actions = ['up', 'down', 'right', 'left', 'stay']\n",
    "        self.q_values = np.zeros(k)  # Estimated reward values\n",
    "        self.action_counts = np.zeros(k)  # Count of actions taken\n",
    "        self.total_reward = 0\n",
    "        self.change_probability = change_probability\n",
    "        self.current_state = 'same_posture'  # Initial state\n",
    "        # Set the seed for reproducibility\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        # Simulate state transition\n",
    "        if np.random.rand() < self.change_probability:\n",
    "            self.current_state = 'changed_posture'\n",
    "        else:\n",
    "            self.current_state = 'same_posture'\n",
    "\n",
    "        # Define rewards based on state and action\n",
    "        if self.current_state == 'changed_posture':\n",
    "            # Let's assume changing posture is generally good\n",
    "            reward = 1 if action != 4 else -1  # Positive reward for any action except 'stay'\n",
    "        else:\n",
    "            reward = -1 if action != 4 else 1  # Negative reward for any action except 'stay'\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def select_action(self):\n",
    "        # Using epsilon-greedy strategy\n",
    "        epsilon = 0.1\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(range(self.k))  # Explore\n",
    "        else:\n",
    "            return np.argmax(self.q_values)  # Exploit\n",
    "\n",
    "    def update_q_values(self, action, reward):\n",
    "        self.action_counts[action] += 1\n",
    "        alpha = 1 / self.action_counts[action]\n",
    "        self.q_values[action] += alpha * (reward - self.q_values[action])\n",
    "\n",
    "    def run(self):\n",
    "        for _ in range(self.iterations):\n",
    "            action = self.select_action()\n",
    "            reward = self.get_reward(action)\n",
    "            self.update_q_values(action, reward)\n",
    "            self.total_reward += reward\n",
    "\n",
    "        print(f\"Total reward after {self.iterations} iterations: {self.total_reward}\")\n",
    "        return self.q_values\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    bandit = SimplePostureBandit()\n",
    "    q = bandit.run()\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow==2.14.0 tensorflow-probability==0.22.0\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"TensorFlow Probability version:\", tfp.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf_agents.environments as tf_env\n",
    "import tf_agents.specs as tf_specs\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "\n",
    "class FiveArmedBanditEnv(tf_env.py_environment.PyEnvironment):\n",
    "    def __init__(self, user_model_vec, change_probability=0.3, seed=42):\n",
    "        self._num_actions = 5\n",
    "        self._user_model_vec = user_model_vec\n",
    "        self._change_probability = change_probability\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        self._dummy_observation = np.array([0.0, 0.0, 0.0, 0.0, 0.0], dtype=np.float32)\n",
    "        np.random.seed(seed)\n",
    "        super(FiveArmedBanditEnv, self).__init__()\n",
    "\n",
    "    def action_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(), dtype=np.int32, minimum=0, maximum=self._num_actions - 1, name='action')\n",
    "\n",
    "    def observation_spec(self):\n",
    "        return tf_specs.BoundedArraySpec(\n",
    "            shape=(self._num_actions,), dtype=np.float32, minimum=0, name='observation')\n",
    "\n",
    "    def _reset(self):\n",
    "        self._step_count = 0\n",
    "        self._total_reward = 0.0\n",
    "        return ts.restart(observation=self._dummy_observation)\n",
    "    \n",
    "    def _step(self, action):\n",
    "        fatigue = 1 - np.exp(-self._lambda_fatigue * self._step_count)\n",
    "        adjusted_change_probability = self._change_probability * (1 - fatigue)\n",
    "\n",
    "        if np.random.uniform(0, 1) < adjusted_change_probability:\n",
    "            self.current_state = 'changed_posture'\n",
    "        else:\n",
    "            self.current_state = 'same_posture'\n",
    "\n",
    "        # Define rewards based on the state and action\n",
    "        if self.current_state == 'changed_posture':\n",
    "            reward = 1 if action != 4 else -1  # Positive reward for any action except 'stay'\n",
    "        else:\n",
    "            reward = -1 if action != 4 else 1  # Negative reward for any action except 'stay'\n",
    "\n",
    "        self._step_count += 1\n",
    "        self._total_reward += reward\n",
    "\n",
    "        return ts.transition(observation=self._dummy_observation, reward=reward)\n",
    "    \n",
    "    def get_total_reward(self):\n",
    "        return self._total_reward\n",
    "\n",
    "    def get_step_count(self):\n",
    "        return self._step_count\n",
    "\n",
    "def generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range):\n",
    "    user_profiles = []\n",
    "    for _ in range(num_profiles):\n",
    "        base_change_probability = np.random.uniform(*base_change_probability_range)\n",
    "        lambda_fatigue = np.random.uniform(*lambda_fatigue_range)\n",
    "        user_profiles.append((base_change_probability, lambda_fatigue))\n",
    "    return user_profiles\n",
    "\n",
    "class UCB1Agent:\n",
    "    def __init__(self, num_actions, explore_rate=2.0):\n",
    "        self.num_actions = num_actions\n",
    "        self.explore_rate = explore_rate\n",
    "        self.action_counts = np.zeros(num_actions)\n",
    "        self.action_values = np.zeros(num_actions)\n",
    "        self.total_steps = 0\n",
    "\n",
    "    def select_action(self):\n",
    "        ucb_values = self.action_values + self.explore_rate * np.sqrt(np.log(self.total_steps + 1) / (self.action_counts + 1))\n",
    "        return np.argmax(ucb_values)\n",
    "\n",
    "    def update_estimates(self, action, reward):\n",
    "        self.total_steps += 1\n",
    "        self.action_counts[action] += 1\n",
    "        alpha = 1 / self.action_counts[action]\n",
    "        self.action_values[action] += alpha * (reward - self.action_values[action])\n",
    "\n",
    "    def get_current_average_reward(self):\n",
    "        return np.sum(self.action_values * self.action_counts) / np.sum(self.action_counts)\n",
    "\n",
    "def user_simulation(user_profiles, num_iterations=500, explore_rate=2.0):\n",
    "    all_average_rewards = []\n",
    "    all_rmse = []\n",
    "\n",
    "    for base_change_probability, lambda_fatigue in user_profiles:\n",
    "        # Create the environment\n",
    "        env = FiveArmedBanditEnv(change_probability=base_change_probability, lambda_fatigue=lambda_fatigue)\n",
    "\n",
    "        # Create the UCB1 agent\n",
    "        agent = UCB1Agent(num_actions=env.action_spec().maximum + 1, explore_rate=explore_rate)\n",
    "\n",
    "        average_rewards = []\n",
    "        rmse = []\n",
    "\n",
    "        # Training loop\n",
    "        for _ in range(num_iterations):\n",
    "            # Main stepping\n",
    "            action = agent.select_action()\n",
    "            time_step = env.step(action)\n",
    "            agent.update_estimates(action, time_step.reward)\n",
    "\n",
    "            # Logs for plotting\n",
    "            average_rewards.append(agent.get_current_average_reward())\n",
    "            true_action_value = 2 * base_change_probability - 1\n",
    "            rmse.append(np.sqrt(np.mean((np.subtract(agent.action_values, true_action_value))**2)))\n",
    "\n",
    "        all_average_rewards.append(average_rewards)\n",
    "        all_rmse.append(rmse)\n",
    "\n",
    "        print(f\"User profile with base change probability {base_change_probability:.2f} and lambda fatigue {lambda_fatigue:.2f}\")\n",
    "        print(\"True values: {}\".format(true_action_value))\n",
    "        print(\"Value estimates: {}\".format(agent.action_values))\n",
    "        print(\"Action counts: {}\".format(agent.action_counts))\n",
    "\n",
    "    return all_average_rewards, all_rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    num_profiles = 30 # We want enough profiles to see the variance in the results but not too many to stay feasible in the real experiment\n",
    "    base_change_probability_range = (0.1, 0.5) # At minimum, the user changes posture 10% of the time and at maximum 50% of the time\n",
    "    lambda_fatigue_range = (0.005, 0.1) # In the paper the values chosen were 0.01, 0.03 and 0.05\n",
    "    \n",
    "    user_profiles = generate_user_profiles(num_profiles, base_change_probability_range, lambda_fatigue_range)\n",
    "    average_rewards, rmse = user_simulation(user_profiles)\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    for i, (avg_rewards, rms) in enumerate(zip(average_rewards, rmse)):\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(avg_rewards, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward over Time')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(rms, label=f'Profile {i+1}')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.title('RMSE over Time')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
